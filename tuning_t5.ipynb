{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Importazione delle librerie\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Descrizione:\n",
    "Iniziamo importando le librerie necessarie per il nostro progetto. Questo include:\n",
    "\n",
    "torch per il lavoro con PyTorch,\n",
    "transformers per il caricamento e utilizzo del modello T5,\n",
    "pandas per la gestione dei dati,\n",
    "tqdm per monitorare il progresso durante l'addestramento,\n",
    "sklearn per suddividere il dataset in training e validation,\n",
    "evaluate per il calcolo delle metriche, come il punteggio BLEU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler\n",
    "from transformers import T5TokenizerFast, T5ForConditionalGeneration\n",
    "from torch.optim import Adam\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "import evaluate "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Caricamento del modello e del tokenizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descrizione:\n",
    "Carichiamo il tokenizer e il modello pre-addestrato T5. Il modello t5-base è preconfigurato per il task di generazione di testo, e il tokenizer converte il testo in sequenze di token compatibili con il modello."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKENIZER = T5TokenizerFast.from_pretrained(\"t5-base\")\n",
    "MODEL = T5ForConditionalGeneration.from_pretrained(\"t5-base\", return_dict=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Caricamento dei dati"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descrizione:\n",
    "Qui carichiamo i dati da un file JSON. Ogni oggetto JSON rappresenta un'istanza con domanda + contesto (input) e una risposta (output)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('new_t5_dataset.json', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Preparazione dei dati"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descrizione:\n",
    "La funzione prepare_data() elabora i dati JSON, separando la domanda (question) e il contesto (context) dal campo \"input\". La domanda e il contesto vengono estratti e organizzati in un formato utile per il modello."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(data):\n",
    "    articles = []\n",
    "    \n",
    "    for item in data:\n",
    "        input_text = item[\"input\"]\n",
    "        output_text = item[\"output\"]\n",
    "        \n",
    "        question = input_text.split(\"Contesto:\")[0].replace(\"Domanda:\", \"\").strip()\n",
    "        context = input_text.split(\"Contesto:\")[1].strip()\n",
    "        \n",
    "        articles.append({\"question\": question, \"context\": context, \"answer\": output_text})\n",
    "\n",
    "    return articles\n",
    "\n",
    "data = prepare_data(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Creazione di un DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descrizione:\n",
    "I dati elaborati vengono convertiti in un DataFrame di pandas per una gestione più semplice e per avere una panoramica dei dati."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            question  \\\n",
      "0                                    Cos'è il MiCAR?   \n",
      "1                         Perché è stato introdotto?   \n",
      "2                              A cosa mira il MiCAR?   \n",
      "3                                 Cosa sono gli EMT?   \n",
      "4  Qual è l'approccio normativo di MiCAR riguardo...   \n",
      "\n",
      "                                             context  \\\n",
      "0  Il MiCAR è il Regolamento sui Mercati delle Cr...   \n",
      "1  Il co-legislatore europeo ha introdotto questa...   \n",
      "2  Considerando che la politica monetaria della B...   \n",
      "3  Gli EMT sono una delle tre categorie di cripto...   \n",
      "4  L'approccio normativo di MiCAR non è quello di...   \n",
      "\n",
      "                                              answer  \n",
      "0  Il MiCAR è il Regolamento sui Mercati delle Cr...  \n",
      "1  Il co-legislatore europeo ha introdotto questa...  \n",
      "2  Considerando che la politica monetaria della B...  \n",
      "3  Gli EMT sono una delle tre categorie di cripto...  \n",
      "4  Nell'analizzare il regime giuridico degli EMT,...  \n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(data)\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Creazione del Dataset personalizzato\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descrizione:\n",
    "Definiamo un dataset personalizzato QA_Dataset che si occupa della tokenizzazione delle domande e risposte, gestendo anche il padding e la creazione dei mascheramenti necessari per l'addestramento del modello T5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QA_Dataset(Dataset):\n",
    "    def __init__(self, tokenizer, dataframe, q_len, t_len):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.q_len = q_len\n",
    "        self.t_len = t_len\n",
    "        self.data = dataframe\n",
    "        self.questions = self.data[\"question\"]\n",
    "        self.context = self.data[\"context\"]\n",
    "        self.answer = self.data['answer']\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.questions)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        question = self.questions[idx]\n",
    "        context = self.context[idx]\n",
    "        answer = self.answer[idx]\n",
    "        \n",
    "        question_tokenized = self.tokenizer(question, context, max_length=self.q_len, padding=\"max_length\",\n",
    "                                            truncation=True, add_special_tokens=True)\n",
    "        answer_tokenized = self.tokenizer(answer, max_length=self.t_len, padding=\"max_length\", \n",
    "                                          truncation=True, add_special_tokens=True)\n",
    "        \n",
    "        labels = torch.tensor(answer_tokenized[\"input_ids\"], dtype=torch.long)\n",
    "        labels[labels == 0] = -100\n",
    "        \n",
    "        return {\n",
    "            \"input_ids\": torch.tensor(question_tokenized[\"input_ids\"], dtype=torch.long),\n",
    "            \"attention_mask\": torch.tensor(question_tokenized[\"attention_mask\"], dtype=torch.long),\n",
    "            \"labels\": labels,\n",
    "            \"decoder_attention_mask\": torch.tensor(answer_tokenized[\"attention_mask\"], dtype=torch.long)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Definizione dei parametri e del DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descrizione:\n",
    "Inizializziamo vari parametri:\n",
    "\n",
    "Q_LEN e T_LEN sono la lunghezza massima delle domande e risposte.\n",
    "BATCH_SIZE determina il numero di esempi per batch.\n",
    "DEVICE indica se utilizzare la CPU o la GPU.\n",
    "Suddividiamo il dataset in training e validation e creiamo i dataloader che permetteranno di iterare sui dati durante l'addestramento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_LEN = 256   # Question Length                                                                                                           \n",
    "T_LEN = 64    # Target Length\n",
    "BATCH_SIZE = 4\n",
    "DEVICE =\"cpu\"  \n",
    "\n",
    "train_data, val_data = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "train_sampler = RandomSampler(train_data.index)\n",
    "val_sampler = RandomSampler(val_data.index)\n",
    "\n",
    "qa_dataset = QA_Dataset(TOKENIZER, df, Q_LEN, T_LEN)\n",
    "\n",
    "train_loader = DataLoader(qa_dataset, batch_size=BATCH_SIZE, sampler=train_sampler)\n",
    "val_loader = DataLoader(qa_dataset, batch_size=BATCH_SIZE, sampler=val_sampler)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Ottimizzazione e addestramento\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descrizione:\n",
    "Definiamo l'ottimizzatore (Adam) e iniziamo l'addestramento per 5 epoche (modificabili). Durante l'addestramento, monitoriamo la perdita di addestramento e di validazione."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training batches:   0%|          | 0/11 [00:00<?, ?it/s]Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
      "Training batches: 100%|██████████| 11/11 [28:05<00:00, 153.25s/it] \n",
      "Validation batches: 100%|██████████| 3/3 [00:08<00:00,  2.83s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/5 -> Train loss: 1.5395323417403481\tValidation loss: 1.2986692388852437\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training batches: 100%|██████████| 11/11 [02:28<00:00, 13.50s/it]\n",
      "Validation batches: 100%|██████████| 3/3 [00:08<00:00,  2.80s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/5 -> Train loss: 1.4565239927985452\tValidation loss: 1.212925910949707\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training batches: 100%|██████████| 11/11 [03:14<00:00, 17.70s/it]\n",
      "Validation batches: 100%|██████████| 3/3 [00:07<00:00,  2.65s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/5 -> Train loss: 1.3970363049796133\tValidation loss: 1.145251366827223\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training batches: 100%|██████████| 11/11 [02:47<00:00, 15.21s/it]\n",
      "Validation batches: 100%|██████████| 3/3 [00:09<00:00,  3.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/5 -> Train loss: 1.3448210968212648\tValidation loss: 1.1019228001435597\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training batches: 100%|██████████| 11/11 [02:04<00:00, 11.33s/it]\n",
      "Validation batches: 100%|██████████| 3/3 [00:08<00:00,  2.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 -> Train loss: 1.29934236461466\tValidation loss: 1.0606210728486378\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "OPTIMIZER = Adam(MODEL.parameters(), lr=0.00001)\n",
    "\n",
    "train_loss = 0\n",
    "val_loss = 0\n",
    "train_batch_count = 0\n",
    "val_batch_count = 0\n",
    "\n",
    "for epoch in range(5):  # Puoi modificare il numero di epoche\n",
    "    MODEL.train()\n",
    "    for batch in tqdm(train_loader, desc=\"Training batches\"):\n",
    "        input_ids = batch[\"input_ids\"].to(DEVICE)\n",
    "        attention_mask = batch[\"attention_mask\"].to(DEVICE)\n",
    "        labels = batch[\"labels\"].to(DEVICE)\n",
    "        decoder_attention_mask = batch[\"decoder_attention_mask\"].to(DEVICE)\n",
    "\n",
    "        outputs = MODEL(input_ids=input_ids, attention_mask=attention_mask, labels=labels,\n",
    "                        decoder_attention_mask=decoder_attention_mask)\n",
    "\n",
    "        OPTIMIZER.zero_grad()\n",
    "        outputs.loss.backward()\n",
    "        OPTIMIZER.step()\n",
    "        train_loss += outputs.loss.item()\n",
    "        train_batch_count += 1\n",
    "    \n",
    "    MODEL.eval()\n",
    "    for batch in tqdm(val_loader, desc=\"Validation batches\"):\n",
    "        input_ids = batch[\"input_ids\"].to(DEVICE)\n",
    "        attention_mask = batch[\"attention_mask\"].to(DEVICE)\n",
    "        labels = batch[\"labels\"].to(DEVICE)\n",
    "        decoder_attention_mask = batch[\"decoder_attention_mask\"].to(DEVICE)\n",
    "\n",
    "        outputs = MODEL(input_ids=input_ids, attention_mask=attention_mask, labels=labels,\n",
    "                        decoder_attention_mask=decoder_attention_mask)\n",
    "\n",
    "        val_loss += outputs.loss.item()\n",
    "        val_batch_count += 1\n",
    "        \n",
    "    print(f\"{epoch+1}/{5} -> Train loss: {train_loss / train_batch_count}\\tValidation loss: {val_loss/val_batch_count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Salvataggio del modello"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descrizione:\n",
    "Dopo l'addestramento, salviamo il modello e il tokenizer per poterli riutilizzare in futuro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('qa_model\\\\tokenizer_config.json',\n",
       " 'qa_model\\\\special_tokens_map.json',\n",
       " 'qa_model\\\\spiece.model',\n",
       " 'qa_model\\\\added_tokens.json',\n",
       " 'qa_model\\\\tokenizer.json')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODEL.save_pretrained(\"qa_model\")\n",
    "TOKENIZER.save_pretrained(\"qa_model\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Funzione di previsione delle risposte"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descrizione:\n",
    "Infine, la funzione predict_answer() utilizza il modello addestrato per generare risposte a una domanda data un contesto. Se viene fornita una risposta di riferimento, calcola anche la metrica BLEU per valutare la qualità della risposta generata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_answer(context, question, ref_answer=None):\n",
    "    inputs = TOKENIZER(question, context, max_length=Q_LEN, padding=\"max_length\", truncation=True, add_special_tokens=True)\n",
    "    \n",
    "    input_ids = torch.tensor(inputs[\"input_ids\"], dtype=torch.long).to(DEVICE).unsqueeze(0)\n",
    "    attention_mask = torch.tensor(inputs[\"attention_mask\"], dtype=torch.long).to(DEVICE).unsqueeze(0)\n",
    "\n",
    "    outputs = MODEL.generate(input_ids=input_ids, attention_mask=attention_mask)\n",
    "  \n",
    "    predicted_answer = TOKENIZER.decode(outputs.flatten(), skip_special_tokens=True)\n",
    "    \n",
    "    if ref_answer:\n",
    "        bleu = evaluate.load(\"google_bleu\")\n",
    "        score = bleu.compute(predictions=[predicted_answer], references=[ref_answer])\n",
    "    \n",
    "        print(\"Context: \\n\", context)\n",
    "        print(\"\\n\")\n",
    "        print(\"Question: \\n\", question)\n",
    "        return {\n",
    "            \"Reference Answer: \": ref_answer, \n",
    "            \"Predicted Answer: \": predicted_answer, \n",
    "            \"BLEU Score: \": score\n",
    "        }\n",
    "    else:\n",
    "        return predicted_answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context: \n",
      " Il co-legislatore europeo ha introdotto questa nuova regolamentazione per diverse ragioni. Prima di tutto, era urgente colmare quella che era stata definita un'\"assenza di un quadro normativo stabile\" sia dall'ESMA (2019) che dall'EBA (2019). Le preoccupazioni sui rischi per i consumatori erano presenti poiché le cripto-attività non rientravano chiaramente nella definizione di strumenti finanziari (e, di conseguenza, nel campo di applicazione della MiFID II), lasciando un vuoto normativo. Inoltre, il legislatore mirava a evitare l'arbitraggio normativo tra i diversi Stati membri dell'UE: data la natura transfrontaliera degli asset basati su DLT, un approccio paese per paese sarebbe stato quasi certamente (e di fatto lo era) insufficiente. Infine, l'attività preparatoria da parte del legislatore è iniziata in un periodo in cui il numero di ICO è salito alle stelle. Le stablecoin non avevano garanzie sufficienti per assicurare la possibilità di convertirle in valuta legale. Queste stablecoin non rientravano nel controllo della BCE sull'emissione di denaro, ostacolando la sua capacità (e il suo dovere) di garantire la stabilità finanziaria e dei prezzi. L'introduzione del MiCAR è legata alla necessità di una regolamentazione chiara e uniforme per le cripto-attività nell'UE, evitando rischi per i consumatori e le disuguaglianze tra gli Stati membri.\n",
      "\n",
      "\n",
      "Question: \n",
      " Perché è stato introdotto il micar?\n",
      "{'Reference Answer: ': 'Perché è stato introdotto?', 'Predicted Answer: ': 'Il co-legislatore europeo ha introdotto questa nu', 'BLEU Score: ': {'google_bleu': 0.045454545454545456}}\n"
     ]
    }
   ],
   "source": [
    "answer=predict_answer(\"Il co-legislatore europeo ha introdotto questa nuova regolamentazione per diverse ragioni. Prima di tutto, era urgente colmare quella che era stata definita un'\\\"assenza di un quadro normativo stabile\\\" sia dall'ESMA (2019) che dall'EBA (2019). Le preoccupazioni sui rischi per i consumatori erano presenti poiché le cripto-attività non rientravano chiaramente nella definizione di strumenti finanziari (e, di conseguenza, nel campo di applicazione della MiFID II), lasciando un vuoto normativo. Inoltre, il legislatore mirava a evitare l'arbitraggio normativo tra i diversi Stati membri dell'UE: data la natura transfrontaliera degli asset basati su DLT, un approccio paese per paese sarebbe stato quasi certamente (e di fatto lo era) insufficiente. Infine, l'attività preparatoria da parte del legislatore è iniziata in un periodo in cui il numero di ICO è salito alle stelle. Le stablecoin non avevano garanzie sufficienti per assicurare la possibilità di convertirle in valuta legale. Queste stablecoin non rientravano nel controllo della BCE sull'emissione di denaro, ostacolando la sua capacità (e il suo dovere) di garantire la stabilità finanziaria e dei prezzi. L'introduzione del MiCAR è legata alla necessità di una regolamentazione chiara e uniforme per le cripto-attività nell'UE, evitando rischi per i consumatori e le disuguaglianze tra gli Stati membri.\",\"Perché è stato introdotto il micar?\",\"Perché è stato introdotto?\")\n",
    "print(answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
